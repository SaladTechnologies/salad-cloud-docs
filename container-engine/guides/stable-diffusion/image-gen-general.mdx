---
title: "Image Generation On SaladCloud"
description: "How to deploy an image generation service on SaladCloud"
---

## High Level

Regardless of your choice of stable diffusion inference server, models, or extensions, the basic process is as follows:

1. Get a docker image that runs your inference server
2. Copy any models and extensions you want into the docker image
3. Enable some way to access your container group, either through the container gateway, or a job queue.
4. Push the new image up to a container registry
5. Deploy the image as a salad container group

## Validated Docker Images
Here are some popular stable diffusion inference servers that weâ€™ve verified work on Salad.
> **Note that you will be interacting with these as an API, and not through their browser user interface.**
  1. ComfyUI
      1. Guide: [ComfyUI](/container-engine/guides/stable-diffusion/basic-how-to-deploy-stable-diffusion-on-salad-comfy)
      1. Git Repo: [https://github.com/ai-dock/comfyui](https://github.com/ai-dock/comfyui)
      1. Docker Image: [ghcr.io/ai-dock/comfyui:latest-cuda](http://ghcr.io/ai-dock/comfyui:latest-cuda)
      1. Model Directory: `/opt/ComfyUI/models`
      1. Custom Node Directory: `/opt/ComfyUI/custom_nodes/`
  2. Automatic1111
      1. Guide: [Automatic1111](/container-engine/guides/stable-diffusion/basic-how-to-deploy-stable-diffusion-on-salad-a1111)
      1. Git Repo: [https://github.com/SaladTechnologies/stable-diffusion-webui-docker](https://github.com/SaladTechnologies/stable-diffusion-webui-docker)
      1. Docker Image: `saladtechnologies/a1111:ipv6-latest` 
      1. Data Directory: `/data`
      1. Model Directory: `/data/models`
      1. Extension Directory: `/data/config/auto/extensions`
      1. Controlnet Model Directory (If controlnet extension is installed): `/data/config/auto/extensions/sd-webui-controlnet/models`
  3. SD.Next
      1. Guide: [SD.Next](/container-engine/guides/stable-diffusion/basic-how-to-deploy-stable-diffusion-on-salad-sdnext)
      1. Git Repo: [https://github.com/SaladTechnologies/sdnext](https://github.com/SaladTechnologies/sdnext)
      1. Docker Image: `saladtechnologies/sdnext:base`
      1. Data Directory: `/webui/data`
      4. Model Directory: `/webui/data/models`
      5. Extension Directory: `/webui/data/extensions`
      6. Controlnet Model Directory: `/webui/extensions-builtin/sd-webui-controlnet/models`
  4. Fooocus:
      1. Guide: [Fooocus](/container-engine/guides/stable-diffusion/basic-how-to-deploy-stable-diffusion-on-salad-fooocus)
      1. Git Repo: [https://github.com/mrhan1993/Fooocus-API](https://github.com/mrhan1993/Fooocus-API)
      2. Docker Image: `konieshadow/fooocus-api:v0.4.1.1`
      3. Model Directory: `/app/repositories/Fooocus/models`
    
## Container Gateway or Job Queue?

When deploying your stable diffusion inference server, you have two options for how to access it:

1. **Container Gateway**: This is the easiest way to access your container group. 
    1. It can be enabled during container group creation, and maps a public https URL to a specific IPv6 port in your container.
    1. You can optionally enable auth, which then requires your Salad API Token to access the container group.
    1. Only requires your server to listen on IPv6, no additional binary.
    1. Salad's Container Gateway uses least-connection load balancing, so you can scale your container group up or down without needing to change the URL.
    1. Completed image generations are returned to the client in the response.
    1. However, it is **not suitable for long-running tasks**, as the container gateway will time out after 100 seconds, a hard limit imposed by Cloudflare on idle connections.
    For most image generation workloads, this is not a problem, as generations typically take under 30s.
    1. **Does Not Retry Failed Requests**: If a request fails, the client must retry the request.
    1. Excess load will result in failed requests, as the container gateway will not queue requests.
2. **Job Queue**: This is a more resilient way to access your container group.
    1. You can submit jobs to a queue, and the container group will process them in order.
    1. You must add a binary to your container image that connects the job queue to your inference server.
    1. High utililization of the container group will result in longer queue times, but no failed requests.
    1. **Retries Failed Requests**: If a request fails, the job queue will retry the request 3 times.
    1. **Long-Running Tasks**: The job queue is suitable for long-running tasks, as it does not have a timeout.
    1. **Fully asyncronous**: Completed jobs are not returned to the client, but instead must be fetched from the job queue, or received by a webhook.

There are several factors you should consider when choosing between the two options:

1. **How predictable is load?**
Scaling services with many gigabytes of models can not be done instantly, so if you expect sudden spikes in traffic, you may want to use a job queue.
On the other hand, if you have very predictable load, the container gateway is easier to use, and you can simply scale your container group up or down as needed with about an hour of lead-time.
1. **How long do you expect your tasks to take?**
Most image generation tasks take just a few seconds on a powerful GPU, but if you have a particularly complex multi-modal workflow, and are using older, less expensive GPUs, you may end up with generation times approaching the cloudflare timeout limit of 100s.
1. **What is the cost structure for your client application?**
If you are using a serverless architecture, you should check to see if you are billed for CPU time, or clock time for your function execution.
If you are billed for clock time, you will likely want to use the job queue, as you will not be billed for time spent waiting in the queue.
If you are billed for CPU time, there will be little difference between the two options.